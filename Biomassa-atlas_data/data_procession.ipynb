{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b706ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import MultiPolygon, Polygon, Point\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import distance\n",
    "from ast import literal_eval\n",
    "import math \n",
    "from geopy.distance import distance\n",
    "import pyproj\n",
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab970e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, title):\n",
    "    \n",
    "    # Separate x and y values from the coordinate tuples and get masses of each point\n",
    "    x_values = [row['Biomass location'][0] for _, row in df.iterrows()]\n",
    "    y_values = [row['Biomass location'][1] for _, row in df.iterrows()]\n",
    "    mass_values = df['Mass'].values\n",
    "\n",
    "    # To plot the coordinates in WGS84-system\n",
    "    \n",
    "    # Define the WGS84 and EUREF-FIN coordinate systems\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    euref_fin = pyproj.CRS('EPSG:3067')\n",
    "    \n",
    "    # Transform the location to WGS84 coordinates\n",
    "    transformer = pyproj.Transformer.from_crs(euref_fin, wgs84, always_xy=True)\n",
    "    x_values, y_values = transformer.transform(x_values,y_values)\n",
    "    \n",
    "    # Plot the data with the colors based on mass\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sc = ax.scatter(x_values, y_values, c=mass_values, cmap='viridis', s=100)\n",
    "    plt.xlabel('Longitude', fontsize=14)\n",
    "    plt.ylabel('Latitude', fontsize=14)\n",
    "    plt.title(' '.join(title.split(' ')[:4]) + '\\n' + ' '.join(title.split(' ')[4:]), fontsize=16)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    \n",
    "    # Add a colorbar to show the mass values\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.ax.set_ylabel('Mass (tonns)', fontsize=14)\n",
    "    \n",
    "    path = './pdfs/Biomasses/'\n",
    "    \n",
    "    if 'within the given radius' in title:\n",
    "        path = './pdfs/Biomasses within the given radius to the biogas facility/'\n",
    "    \n",
    "    fig.savefig(path + title+'.pdf')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640fa5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df,weighted_avg_centroids,sum_cluster_masses,pdfname):\n",
    "    \n",
    "    masses_in_truck_capacity = np.array(sum_cluster_masses) / 45\n",
    "    \n",
    "    # Set up a colormap to color the centroids based on their sum of masses\n",
    "    cmap = plt.get_cmap('Reds')\n",
    "    norm = mcolors.LogNorm(vmin=np.array(masses_in_truck_capacity).min(), vmax=np.array(masses_in_truck_capacity).max())\n",
    "    #norm = plt.Normalize(np.array(masses_in_truck_capacity).min(), np.array(masses_in_truck_capacity).max())\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    locations = df['Biomass location'].tolist()\n",
    "    locations = np.array(locations)\n",
    "    centroids = np.array(weighted_avg_centroids)\n",
    "    \n",
    "    # To plot the coordinates in WGS84-system,\n",
    "    # define the WGS84 and EUREF-FIN coordinate systems\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    euref_fin = pyproj.CRS('EPSG:3067')\n",
    "    \n",
    "    # Transform the location to WGS84 coordinates\n",
    "    transformer = pyproj.Transformer.from_crs(euref_fin, wgs84, always_xy=True)\n",
    "    x_values, y_values = transformer.transform(locations[:, 0],locations[:, 1])\n",
    "    x_centroids, y_centroids = transformer.transform(centroids[:, 0],centroids[:, 1])\n",
    "\n",
    "    # Plotting the data with the centroids\n",
    "\n",
    "    # scatter plot of data points and their clusters\n",
    "    plt.scatter(x_values, y_values, c=df['Cluster'])\n",
    "\n",
    "    # scatter plot of cluster centroids colored by their sum of masses\n",
    "    plt.scatter(x_centroids, y_centroids, marker='.', s=200, linewidths=3, c=masses_in_truck_capacity, cmap=cmap, norm=norm)\n",
    "\n",
    "    # Add a colorbar to show the logarithmic scale colormap legend\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.set_ylabel('Masses in truckloads (45 tons)')\n",
    "    cbar.set_ticks([1, 10, 100, 1000])  # Customize the ticks on the colorbar\n",
    "    \n",
    "    plt.xlabel('Longitude', fontsize=14)\n",
    "    plt.ylabel('Latitude', fontsize=14)\n",
    "    plt.title('Pickup sites for the file '+' '.join(filename.split(' ')[:4]) + '\\n' + ' '.join(filename.split(' ')[4:]), fontsize=16)\n",
    "\n",
    "    path = './pdfs/Raw pickup sites/'\n",
    "\n",
    "    if 'Thinned' in pdfname:\n",
    "        path = './pdfs/Thinned out pickup sites/'\n",
    "    \n",
    "    fig.savefig(path + pdfname +filename+'.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b4e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(filename):\n",
    "    # Read the CSV file and specify the data types of the columns\n",
    "    dtypes = {'Area': str}\n",
    "    df = pd.read_csv(filename, header=0, sep=';', dtype=dtypes)\n",
    "\n",
    "    # Replace characters in the Area column and split it into separate columns\n",
    "    replacements = {'(': '', ')': '', 'MULTIPOLYGON': ''}\n",
    "    for index1, row in df.iterrows():\n",
    "        input_string = str(df['Area'][index1])\n",
    "        for old, new in replacements.items():\n",
    "            input_string = input_string.replace(old, new)\n",
    "        coordinates = input_string.split(',')\n",
    "        for index2, coordinate in enumerate(coordinates):\n",
    "            df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
    "\n",
    "    # Calculate the mean points of biomass potentials and replace the coordinates with them\n",
    "    df2 = df[['Coordinate 0','Coordinate 1','Coordinate 2','Coordinate 3']]\n",
    "    df2 = df2.apply(lambda row: tuple(map(lambda x: np.sum(x)/len(x), zip(*[map(float, str(c).split()) for c in row]))), axis=1)\n",
    "    df['Biomass location'] = df2\n",
    "\n",
    "    # Clean the dataframe\n",
    "    df = df.loc[:, ['Mass','Biomass location']]\n",
    "    mask = df['Biomass location'].apply(is_2d_tuple)\n",
    "    df = df[mask]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0246c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df,radius,facilityloc):\n",
    "\n",
    "    # Clean the data (df) from the rows that are further away than the assumed maximum distance (radius)\n",
    "    # from the biogas facility locations (facilityloc). \n",
    "\n",
    "    # Check input parameters\n",
    "    if not isinstance(facilityloc, tuple) or len(facilityloc) != 2 or \\\n",
    "       not all(isinstance(x, (int, float)) for x in facilityloc):\n",
    "        raise ValueError('facilityloc must be a tuple with two numeric elements')\n",
    "\n",
    "    # Define the WGS84 and EUREF-FIN coordinate systems\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    euref_fin = pyproj.CRS('EPSG:3067')\n",
    "    \n",
    "    # Transform the facility location to EUREF-FIN coordinates\n",
    "    transformer = pyproj.Transformer.from_crs(wgs84, euref_fin, always_xy=True)\n",
    "    facility_euref_x, facility_euref_y = transformer.transform(facilityloc[1], facilityloc[0])\n",
    "    facility_loc = (facility_euref_x, facility_euref_y)\n",
    " \n",
    "    \n",
    "    # Calculate the distances\n",
    "    distances = df['Biomass location'].apply(lambda loc: math.sqrt(pow(facility_loc[0]-loc[0],2)+pow(facility_loc[1]-loc[1],2))/1000)\n",
    "\n",
    "    # Filter the data\n",
    "    df = df.loc[distances <= radius].reset_index(drop=True)\n",
    "    \n",
    "    # Remove zero masses from data\n",
    "    df = df.loc[(df != 0).all(axis=1)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937369f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cluster_masses_and_locations(df):\n",
    "\n",
    "    # Getting masses and locations for each cluster\n",
    "\n",
    "    cluster_masses = [[] for _ in range(len(df['Cluster'].unique()))]\n",
    "    cluster_locations = [[] for _ in range(len(df['Cluster'].unique()))]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cluster_masses[row['Cluster']].append(row['Mass'])\n",
    "        cluster_locations[row['Cluster']].append(row['Biomass location'])\n",
    "\n",
    "    return cluster_masses, cluster_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb7072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sums_and_weight_avg_centroids(df,cluster_masses,cluster_locations):\n",
    "    # Calculate the weighted average centroid for dataframe's clusters:\n",
    "\n",
    "    weighted_avg_centroids = []\n",
    "    sum_cluster_masses = []\n",
    "\n",
    "    for i in range(len(df['Cluster'].unique())):\n",
    "        sum_x = np.sum([mass * loc[0] for mass, loc in zip(cluster_masses[i], cluster_locations[i])])\n",
    "        sum_y = np.sum([mass * loc[1] for mass, loc in zip(cluster_masses[i], cluster_locations[i])])\n",
    "        weighted_avg_centroids.append((sum_x / np.sum(cluster_masses[i]), sum_y / np.sum(cluster_masses[i])))\n",
    "        sum_cluster_masses.append(np.sum(cluster_masses[i]))\n",
    "\n",
    "    return weighted_avg_centroids, sum_cluster_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f808f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_2d_tuple(val):\n",
    "    \n",
    "    # Check whether val given as parameter is in expected form of tuple. is used by load_and_process data.\n",
    "    \n",
    "    return isinstance(val, tuple) and len(val) == 2 and all(isinstance(x, (int, float)) for x in val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fd5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_loc = None \n",
    "greatest_distance = None\n",
    "\n",
    "def set_facility_loc_and_rad():\n",
    "    global facility_loc\n",
    "    global greatest_distance  \n",
    "    user_input = input(\"Give the coordinates of facility in inspection as a tuple: \")\n",
    "    trimmed_input = user_input.strip(\"())\")\n",
    "    coordinates = trimmed_input.split(\",\")\n",
    "    coordinates = [float(x) for x in coordinates]\n",
    "    facility_loc = tuple(coordinates)\n",
    "    greatest_distance = int(input(\"Give greatest distance of what biomasses are included: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6867b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_clusters(df,tire_thickness,tires,cluster_divisor_coefficient,facility_loc):\n",
    "    \n",
    "    # Define the WGS84 and EUREF-FIN coordinate systems\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    euref_fin = pyproj.CRS('EPSG:3067')\n",
    "    \n",
    "    # Transform the facility location to EUREF-FIN coordinates\n",
    "    transformer = pyproj.Transformer.from_crs(wgs84, euref_fin, always_xy=True)\n",
    "    facility_euref_x, facility_euref_y = transformer.transform(facility_loc[1], facility_loc[0])\n",
    "    facility_loc = (facility_euref_x, facility_euref_y)\n",
    "    \n",
    "    # Calculate the distances\n",
    "    distances = df['Biomass location'].apply(lambda loc: math.sqrt(pow(facility_loc[0]-loc[0],2)+pow(facility_loc[1]-loc[1],2))/1000)\n",
    "    #i_values = {}  # Create an empty dictionary to store the separated dataframe names and their values\n",
    "    i_values = [None]*tires\n",
    "    \n",
    "    for i in range(tires,0,-1): # Reversed looping for handling the tires in a smarter way\n",
    "        \n",
    "        # Reordering the data to subsets of tires\n",
    "        i_values[tires-i] = df[(tire_thickness*(i-1) <= distances) & (distances <= tire_thickness*i)]  # Assign the filtered data to the corresponding tire\n",
    "\n",
    "        # Coordinates with mass coefficients of a data subset \n",
    "        coords = i_values[tires-i]['LocationWithMassCoefficient'].explode().tolist()\n",
    "\n",
    "        LocationMassCoords = [(coords[i], coords[i+1]) for i in range(0, len(coords)-1, 2)]\n",
    "        LocationMassCoords = np.array(LocationMassCoords)\n",
    "\n",
    "        # CLustering the data of a subset with k-value defined by cluster_divisor_coefficient. Thinning the cluster-areas from \n",
    "        # greater distance. Saving new clusters to ThinnedCLusters columns\n",
    "        previous_kluster_amount = len(i_values[tires-i]['Cluster'].unique()) # NOTE: IN ORIGINAL DATA, ONE CLUSTER MAY BE LOCATED TO MULTIPLE TIRE\n",
    "                                                                             # THIS SHOULD BE TAKEN IN NOTICE IN DECIDING cluster_divisor_coefficient     \n",
    "        kmeans = KMeans(n_clusters=int(previous_kluster_amount*pow(cluster_divisor_coefficient,i-1)),n_init='auto')\n",
    "        kmeans.fit(LocationMassCoords)\n",
    "\n",
    "        i_values[tires-i]['Cluster'] = kmeans.predict(i_values[tires-i]['Biomass location'].tolist())\n",
    "\n",
    "    return i_values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cba4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite_dataframes_of_thinned_cluster_areas(dfs):\n",
    "\n",
    "    # Assuming your dataframes are stored in a list called \"dfs\"\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            combined_df = df\n",
    "            continue\n",
    "        \n",
    "        clustersAdded = int(len(combined_df['Cluster'].unique()))\n",
    "        \n",
    "        df['Cluster'] = df['Cluster']+clustersAdded\n",
    "        \n",
    "        combined_df = pd.concat([combined_df,df])\n",
    "        \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f86cd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_biomasses_with_KMeans(df,k):\n",
    "    \n",
    "    # To duplicate the coordinate points based on their masses to consider\n",
    "    # the geographical distribution of masses in clustering.\n",
    "    \n",
    "    df['MassCoefficient'] = df['Mass'].apply(round).astype(int).clip(lower=1)\n",
    "    df['LocationWithMassCoefficient'] = df['Biomass location'] * df['MassCoefficient']\n",
    "    coords = df['LocationWithMassCoefficient'].explode().tolist()\n",
    "    LocationMassCoords = [(coords[i], coords[i+1]) for i in range(0, len(coords)-1, 2)]\n",
    "    LocationMassCoords = np.array(LocationMassCoords)    \n",
    "\n",
    "    # fitting the mass-distribution fixed data with KMeans\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k,n_init='auto')\n",
    "    kmeans.fit(LocationMassCoords)\n",
    "    \n",
    "    locations = df['Biomass location'].tolist()\n",
    "    locations = np.array(locations)\n",
    "\n",
    "    # Making predictions of cluster for each coordinate point \n",
    "    df['Cluster'] = kmeans.predict(locations)\n",
    "    \n",
    "    return df, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf24395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter biomassdata filename: Nautojen lietelanta varastosta 2015\n",
      "Enter the name of the folder where the file is located: Lietelannat\n",
      "Give the coordinates of facility in inspection as a tuple: (60.929833, 23.369694)\n",
      "Give greatest distance of what biomasses are included: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename, header=0, sep=';', dtype=dtypes)\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 973, in setitem\n",
      "    casted = np_can_hold_element(values.dtype, value)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 2041, in np_can_hold_element\n",
      "    raise LossySetitemError\n",
      "pandas.core.dtypes.cast.LossySetitemError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\470240604.py\", line 5, in <module>\n",
      "    df = load_and_process_data(path+'/'+folder+'/'+filename)\n",
      "  File \"C:\\Users\\K-A-S\\AppData\\Local\\Temp\\ipykernel_15016\\700681997.py\", line 14, in load_and_process_data\n",
      "    df.loc[index1,'Coordinate '+ str(index2)]=coordinate\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\indexing.py\", line 818, in __setitem__\n",
      "    iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1750, in _setitem_with_indexer\n",
      "    self._setitem_with_indexer(new_indexer, value, name)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1795, in _setitem_with_indexer\n",
      "    self._setitem_with_indexer_split_path(indexer, value, name)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1888, in _setitem_with_indexer_split_path\n",
      "    self._setitem_single_column(loc, value, pi)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1992, in _setitem_single_column\n",
      "    self.obj._mgr.column_setitem(loc, plane_indexer, value)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1391, in column_setitem\n",
      "    new_mgr = col_mgr.setitem((idx,), value)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 393, in setitem\n",
      "    return self.apply(\"setitem\", indexer=indexer, value=value)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 352, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 976, in setitem\n",
      "    nb = self.coerce_to_target_dtype(value)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 450, in coerce_to_target_dtype\n",
      "    return self.astype(new_dtype, copy=False)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 526, in astype\n",
      "    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 299, in astype_array_safe\n",
      "    new_values = astype_array(values, dtype, copy=copy)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 230, in astype_array\n",
      "    values = astype_nansafe(values, dtype, copy=copy)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 170, in astype_nansafe\n",
      "    return arr.astype(dtype, copy=True)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.03 MiB for an array with shape (659818,) and data type object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 877, in format_record\n",
      "    _format_traceback_lines(\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 158, in _format_traceback_lines\n",
      "    line = stack_line.render(pygmented=has_colors).rstrip('\\n') + '\\n'\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\stack_data\\core.py\", line 360, in render\n",
      "    start_line, lines = self.frame_info._pygmented_scope_lines\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\stack_data\\core.py\", line 780, in _pygmented_scope_lines\n",
      "    lines = _pygmented_with_ranges(formatter, code, ranges)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\stack_data\\utils.py\", line 165, in _pygmented_with_ranges\n",
      "    return pygments.highlight(code, lexer, formatter).splitlines()\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\__init__.py\", line 82, in highlight\n",
      "    return format(lex(code, lexer), formatter, outfile)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\__init__.py\", line 64, in format\n",
      "    formatter.format(tokens, realoutfile)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\formatters\\terminal256.py\", line 250, in format\n",
      "    return Formatter.format(self, tokensource, outfile)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\formatter.py\", line 124, in format\n",
      "    return self.format_unencoded(tokensource, outfile)\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\formatters\\terminal256.py\", line 256, in format_unencoded\n",
      "    for ttype, value in tokensource:\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\stack_data\\utils.py\", line 158, in get_tokens\n",
      "    for ttype, value in super().get_tokens(text):\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\lexer.py\", line 250, in streamer\n",
      "    for _, t, v in self.get_tokens_unprocessed(text):\n",
      "  File \"C:\\Users\\K-A-S\\anaconda3\\envs\\JODA\\lib\\site-packages\\pygments\\lexer.py\", line 693, in get_tokens_unprocessed\n",
      "    m = rexmatch(text, pos)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "filename = input(\"Enter biomassdata filename: \") + '.csv'\n",
    "folder = input(\"Enter the name of the folder where the file is located: \")\n",
    "path = './datasets/'\n",
    "set_facility_loc_and_rad()\n",
    "df = load_and_process_data(path+'/'+folder+'/'+filename)\n",
    "file = filename.split('.',1)[0]\n",
    "# Coordinates for Envor's facility: (60.929833, 23.369694)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa4ebf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_data(\u001b[43mdf\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBiomasses of \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "plot_data(df,'Biomasses of ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85018165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df,greatest_distance,facility_loc)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df,file+' within the given radius to the biogas facility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tsekkaa paperi GIS-data related route optimization, hierarchical clustering, location\n",
    "# optimization, and kernel density methods are useful for promoting\n",
    "# distributed bioenergy plant planning in rural areas\n",
    "# K. Laasasenahoa*, A. Lensub*, R. Lauhanenc\n",
    "# and J. Rintalaa\n",
    "\n",
    "# Clustering the data with K-means. Utilzing the elbow method to decide optimal value for k.\n",
    "# The basic idea in elbow method is to plot the within-cluster \n",
    "# sum of squares (WCSS) against the number of clusters, and choose the number of clusters \n",
    "# at the \"elbow\" point where the rate of decrease in WCSS slows down significantly.\n",
    "\n",
    "# To be used in consideration of mass distribution. Each coordinate will be multiplied \n",
    "# with their MassCoefficient in clustering\n",
    "# to give more weight to the points with greater mass. \n",
    "\n",
    "df['MassCoefficient'] = df['Mass'].apply(round).astype(int).clip(lower=1)\n",
    "df['LocationWithMassCoefficient'] = df['Biomass location'] * df['MassCoefficient']\n",
    "\n",
    "coords = df['LocationWithMassCoefficient'].explode().tolist()\n",
    "LocationMassCoords = [(coords[i], coords[i+1]) for i in range(0, len(coords)-1, 2)]\n",
    "LocationMassCoords = np.array(LocationMassCoords)\n",
    "\n",
    "'''\n",
    "k_vals = list(range(1,100))\n",
    "wcss = []\n",
    "labels=[]\n",
    "\n",
    "for i in k_vals:    \n",
    "    kmeans = KMeans(n_clusters=i,n_init='auto')\n",
    "    kmeans.fit(LocationMassCoords)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    labels.append(kmeans.predict(LocationMassCoords))\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81393cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the WCSS values against the number of clusters\n",
    "'''\n",
    "plt.plot(range(1, 100), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ea8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the graph above, close to optimal and good enough k-values for this datasets is 60.\n",
    "# Let's plot the data with 60 clusters.\n",
    "# Plotting the data\n",
    "\n",
    "# Because of the later trimming data from the clusters far away from the facility, initialize clustering with K=200\n",
    "\n",
    "#KCLUSTERS = 200\n",
    "\n",
    "#kmeans = KMeans(n_clusters=KCLUSTERS,n_init='auto')\n",
    "#kmeans.fit(LocationMassCoords)\n",
    "#labels = kmeans.predict(LocationMassCoords)\n",
    "#plt.scatter(LocationMassCoords[:,0],LocationMassCoords[:,1],c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, kmeans = cluster_biomasses_with_KMeans(df,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ef332",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_masses, cluster_locations =  df_cluster_masses_and_locations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_avg_centroids, sum_cluster_masses = cluster_sums_and_weight_avg_centroids(df,cluster_masses, cluster_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df,weighted_avg_centroids,sum_cluster_masses,'Raw pickup sites for the file ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_and_clustered_tires = thin_clusters(df,10,5,0.7,facility_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_area = unite_dataframes_of_thinned_cluster_areas(thinned_and_clustered_tires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_cluster_masses, thinned_cluster_locations =  df_cluster_masses_and_locations(thinned_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca183ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_weighted_avg_centroids, thinned_sum_cluster_masses = cluster_sums_and_weight_avg_centroids(thinned_area,thinned_cluster_masses, thinned_cluster_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(thinned_area,thinned_weighted_avg_centroids, thinned_sum_cluster_masses,'Thinned out pickup sites for the file ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME COORDINATE-CONVERSION HANDLING\n",
    "\n",
    "# define the ETRS-TM35FIN projection\n",
    "\n",
    "# CHECK LATER IF THIS IS NEEDED AT ALL\n",
    "\n",
    "etrs_tm35fin = pyproj.Proj(\"+proj=utm +zone=35 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")\n",
    "\n",
    "# define a function to convert EUREF-FIN coordinates to lon-lat\n",
    "def euref_fin_to_lon_lat(row):\n",
    "    lon, lat = etrs_tm35fin(row['ClusterLocations'][0], row['ClusterLocations'][1], inverse=True)\n",
    "    return pd.Series({'lon': lon, 'lat': lat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82936cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERS TO GEOJSONS\n",
    "\n",
    "df = pd.DataFrame({'ClusterLocations': weighted_avg_centroids, 'Clustermasses': sum_cluster_masses}, index=range(1, len(weighted_avg_centroids)+1))\n",
    "\n",
    "df[['lon','lat']] = df.apply(euref_fin_to_lon_lat,axis=1)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "gdf = gdf.drop('ClusterLocations',axis=1) # Not needed anymore, these are in the wrong form of coordinates\n",
    "\n",
    "## save the GeoDataFrame to a GeoJSON file\n",
    "path = './geojsons/'\n",
    "gdf.to_file(path + file+'.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb7df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d1e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
